{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNJbZRFpVrTBEpft/ebw/hO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"t3B-fk4D0AnZ"},"outputs":[],"source":["# @title preprocessing\n","\n","import os\n","import torch\n","# !pip install clip\n","import clip\n","from PIL import Image\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, Dataset\n","\n","# Load the CLIP model\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, preprocess = clip.load('ViT-B/32', device=device)\n","\n","# Define a custom dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, directory, transform=None):\n","        self.directory = directory\n","        self.transform = transform\n","        self.images = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(('.png', '.jpg', '.jpeg'))]\n","        self.images.sort()  # Ensure the order is consistent\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.images[idx]\n","        img = Image.open(img_path).convert('RGB')\n","        if self.transform:\n","            img = self.transform(img)\n","        return img, img_path\n","\n","def create_dataloader(directory, batch_size=32):\n","    transform = transforms.Compose([\n","        transforms.Resize((1000, 1000)),  # Assuming all images are resized to 1000x1000\n","        preprocess\n","    ])\n","    dataset = CustomDataset(directory, transform=transform)\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n","    return dataloader\n","\n","EMOTIONS = list(sorted(['anger', 'disgust', 'fear', 'happiness', 'sadness']))\n"]},{"cell_type":"code","source":["# @title evaluation\n","# Create DataLoaders for input and output images\n","input_image_dir = '/content/with-beautiful-colors_resized_frame_1.jpeg'\n","input_dataloader = create_dataloader(input_image_dir, batch_size=1)\n","\n","output_image_dir = '/content/content/output_folder/with beautiful colors_resized_frame_1_anger-1.jpg'\n","output_dataloader = create_dataloader(output_image_dir, batch_size=1)\n","\n","for (input_image, input_path), (output_image, output_path) in zip(input_dataloader, output_dataloader):\n","        input_image = input_image.to(device)\n","        output_image = output_image.to(device)\n","        # Compute embeddings\n","        with torch.no_grad():\n","            input_embedding = model.encode_image(input_image)\n","            output_embedding = model.encode_image(output_image)\n","for emotion in EMOTIONS:\n","    # Process each pair of input and output images\n","\n","\n","\n","\n","        # Compute cosine similarity\n","        cosine_sim = torch.nn.functional.cosine_similarity(input_embedding, output_embedding, dim=1)\n","\n","        # Print or save the similarity score\n","        print(f\"Comparing {input_path[0]} with {output_path[0]}: Cosine Similarity = {cosine_sim.item()}\")"],"metadata":{"id":"A1IB6wee3-E5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import zipfile\n","# !unzip '/content/output_folder.zip'\n"],"metadata":{"id":"C_m53LDE0tmb"},"execution_count":null,"outputs":[]}]}