{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMoabDaIvQijjPhv9NlSZzu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","# # drive.mount('/content/drive/MyDrive/KAUST/Generative_AI')"],"metadata":{"id":"5Lyx-COAkhBT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install git+https://github.com/openai/CLIP.git\n","\n","import os\n","import zipfile\n","import torch\n","import clip\n","from PIL import Image"],"metadata":{"id":"Fd7PH0T0mXOW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the CLIP model\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SVYRbGtKmiwp","executionInfo":{"status":"ok","timestamp":1715430398478,"user_tz":-180,"elapsed":7638,"user":{"displayName":"Ali Allail","userId":"11589524239520467033"}},"outputId":"3501a288-c428-4369-baf9-c5de890f3ef9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 127MiB/s]\n"]}]},{"cell_type":"code","source":["\n","# Function to extract zip files\n","def extract_zip(input_zip):\n","    input_zip = zipfile.ZipFile(input_zip)\n","    return {name: input_zip.read(name) for name in input_zip.namelist()}\n","\n","# extract_zip('/content/AIF166.zip')\n","# extract_zip('/content/data_images.zip')\n","# !unzip '/content/AIF166.zip'\n","# !unzip '/content/data_images.zip'"],"metadata":{"id":"dfaa6GXXn0Eb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Define the paths to the directories\n","original_images_path = '/content/content/data_images'\n","transformed_images_path = '/content/AIF166'\n","emotions = ['anger', 'happiness', 'sadness', 'fear']\n","\n","def process_image(image_path):\n","    with Image.open(image_path) as img:\n","        return preprocess(img).unsqueeze(0).to(device)\n","\n","# Compute cosine similarity\n","def compute_cosine_similarity(features1, features2):\n","    features1 = features1 / features1.norm(dim=-1, keepdim=True)\n","    features2 = features2 / features2.norm(dim=-1, keepdim=True)\n","    return (features1 * features2).sum(dim=1)\n","\n","# Initialize dictionary to store results\n","results = {}\n","\n","# Process each emotion\n","for emotion in emotions:\n","    emotion_dir = os.path.join(transformed_images_path, emotion)\n","    for img_filename in os.listdir(emotion_dir):\n","        # Remove leading underscore and change extension from .png to .jpg for original images\n","        original_img_filename = img_filename.lstrip('_').replace('.png', '.jpg')\n","        original_img_path = os.path.join(original_images_path, original_img_filename)\n","        transformed_img_path = os.path.join(emotion_dir, img_filename)\n","\n","        if not os.path.exists(original_img_path):\n","            print(f\"Original image not found: {original_img_path}\")\n","            continue\n","        if not os.path.exists(transformed_img_path):\n","            print(f\"Transformed image not found: {transformed_img_path}\")\n","            continue\n","\n","        # Load and process images\n","        original_features = model.encode_image(process_image(original_img_path)).detach()\n","        transformed_features = model.encode_image(process_image(transformed_img_path)).detach()\n","\n","        # Compute similarities\n","        image_similarity = compute_cosine_similarity(original_features, transformed_features).item()\n","\n","        # Compute similarity with the emotion descriptor\n","        text_tokens = clip.tokenize([emotion]).to(device)\n","        emotion_features = model.encode_text(text_tokens).detach()\n","        emotion_similarity = compute_cosine_similarity(transformed_features, emotion_features).item()\n","\n","        # Store results\n","        if emotion not in results:\n","            results[emotion] = {'image_similarities': [], 'emotion_similarities': []}\n","        results[emotion]['image_similarities'].append(image_similarity)\n","        results[emotion]['emotion_similarities'].append(emotion_similarity)\n","\n","# Print organized results\n","for emotion, data in results.items():\n","    avg_img_sim = sum(data['image_similarities']) / len(data['image_similarities'])\n","    avg_emot_sim = sum(data['emotion_similarities']) / len(data['emotion_similarities'])\n","    print(f\"{emotion}: Avg Image Similarity = {avg_img_sim}, Avg Emotion Similarity = {avg_emot_sim}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3j-oe3Lfmky2","executionInfo":{"status":"ok","timestamp":1715431346203,"user_tz":-180,"elapsed":27332,"user":{"displayName":"Ali Allail","userId":"11589524239520467033"}},"outputId":"5ced1316-9af3-499f-9f6a-7e9a314c9a34"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["anger: Avg Image Similarity = 0.8909220867846386, Avg Emotion Similarity = 0.22089431946536145\n","happiness: Avg Image Similarity = 0.8861510495105421, Avg Emotion Similarity = 0.20995358386671686\n","sadness: Avg Image Similarity = 0.8847979809864458, Avg Emotion Similarity = 0.2259624435240964\n","fear: Avg Image Similarity = 0.8877129612198795, Avg Emotion Similarity = 0.22000747129141565\n"]}]}]}